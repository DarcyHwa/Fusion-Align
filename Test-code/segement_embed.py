import requests
import numpy as np
from transformers import AutoTokenizer
import unicodedata
from scipy.spatial.distance import cosine

# ========== å…¨å±€é…ç½® ==========
# XLM-RoBERTa-XL æ¨¡å‹é…ç½®ï¼ˆä»…ç”¨äºåˆ†è¯ï¼‰
MODEL_ID = "facebook/xlm-roberta-xl"

# Qwen3-Embedding-8B API é…ç½®
API_URL = "https://api.siliconflow.cn/v1/embeddings"
API_KEY = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# æ³¨ï¼šç°åœ¨åªä½¿ç”¨åˆ†è¯åŠŸèƒ½ï¼Œä¸éœ€è¦GPUè®¡ç®—

# åŠ è½½ XLM-RoBERTa-XL åˆ†è¯å™¨ï¼ˆä»…ç”¨äºåˆ†è¯ï¼‰
print(f"æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {MODEL_ID}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
print(f"åˆ†è¯å™¨åŠ è½½å®Œæˆ")


def get_local_tokenization(text):
    """ä½¿ç”¨æœ¬åœ° XLM-RoBERTa-XL æ¨¡å‹è¿›è¡Œåˆ†è¯ï¼ˆä¸åŒ…å«ç‰¹æ®Šæ ‡è®°ï¼‰"""
    # è®¾ç½® add_special_tokens=False ç¡®ä¿ä¸åŒ…å«ç‰¹æ®Šæ ‡è®°
    inputs = tokenizer(text, padding=False, truncation=True,
                      max_length=512, add_special_tokens=False)
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'])

    # è§£ç tokenä¸ºå¯è¯»æ–‡æœ¬ï¼Œè¿‡æ»¤æ‰ç©ºçš„æˆ–æ— æ•ˆçš„token
    filtered_tokens = []
    for token in tokens:
        decoded = decode_token(token)
        # åªæ·»åŠ éç©ºçš„ã€æœ‰æ•ˆçš„token
        if decoded is not None and decoded.strip() != "":
            filtered_tokens.append(decoded)

    return filtered_tokens


def is_special_token(token):
    """åˆ¤æ–­æ˜¯å¦ä¸ºç‰¹æ®Šæ ‡è®°"""
    if token in getattr(tokenizer, 'all_special_tokens', []):
        return True
    if token.startswith('<') and token.endswith('>'):
        return True
    try:
        if tokenizer.convert_tokens_to_string([token]).strip() == "":
            return True
    except Exception:
        pass
    return False


def decode_token(token):
    """è§£ç å•ä¸ªtokenä¸ºå¯è¯»æ–‡æœ¬ï¼Œç§»é™¤SentencePieceç‰¹æ®Šæ ‡è®°"""
    try:
        # å…ˆä½¿ç”¨tokenizerè§£ç 
        s = tokenizer.convert_tokens_to_string([token]).strip()
        
        # ç§»é™¤SentencePieceç‰¹æ®Šæ ‡è®°
        # 'â–' æ˜¯SentencePieceä¸­è¡¨ç¤ºè¯å¼€å§‹çš„æ ‡è®°
        s = s.replace('â–', '')
        
        # ç§»é™¤å…¶ä»–å¸¸è§çš„ç‰¹æ®Šæ ‡è®°
        special_marks = ['<s>', '</s>', '<pad>', '<unk>', '<mask>']
        for mark in special_marks:
            s = s.replace(mark, '')
        
        # å»é™¤å‰åç©ºç™½
        s = s.strip()
        
        # å¦‚æœè§£ç åä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè¿”å›Noneè¡¨ç¤ºåº”è¯¥è¿‡æ»¤æ‰è¿™ä¸ªtoken
        return s if s != "" else None
    except Exception:
        # å¦‚æœè§£ç å¤±è´¥ï¼Œæ£€æŸ¥åŸå§‹tokenæ˜¯å¦åŒ…å«ç‰¹æ®Šæ ‡è®°
        if 'â–' in token:
            cleaned_token = token.replace('â–', '').strip()
            return cleaned_token if cleaned_token != "" else None
        return None


def get_qwen_embeddings(text_list):
    """è°ƒç”¨ Qwen3-Embedding-8B æ¨¡å‹è·å–åµŒå…¥å‘é‡"""
    if not text_list:
        return []

    payload = {
        "model": "Qwen/Qwen3-Embedding-8B",
        "input": text_list,
        "encoding_format": "float"
    }

    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

    response = requests.post(API_URL, json=payload, headers=headers, timeout=60)

    try:
        resp_json = response.json()
        embeddings = [item.get("embedding") for item in resp_json.get("data", [])]
    except ValueError:
        raise RuntimeError(f"æ¥å£è¿”å›é JSON å†…å®¹: {response.text[:200]}")

    if len(embeddings) != len(text_list):
        raise RuntimeError("è¿”å›çš„åµŒå…¥æ•°é‡ä¸è¾“å…¥æ–‡æœ¬æ•°é‡ä¸ä¸€è‡´")

    # è½¬æ¢ä¸ºä¸åŸä»£ç å…¼å®¹çš„æ ¼å¼
    class EmbeddingObject:
        def __init__(self, values):
            self.values = values

    return [EmbeddingObject(emb) for emb in embeddings]





def cosine_similarity(vec1, vec2):
    """è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    if not vec1 or not vec2:
        return 0

    # å¤„ç†numpyæ•°ç»„å’Œåˆ—è¡¨
    if hasattr(vec1, 'flatten'):
        vec1 = vec1.flatten()
    if hasattr(vec2, 'flatten'):
        vec2 = vec2.flatten()

    return 1 - cosine(vec1, vec2)


def calculate_similarity_matrix(embeddings1, embeddings2):
    """è®¡ç®—ä¸¤ç»„åµŒå…¥å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ"""
    similarity_matrix = np.zeros((len(embeddings1), len(embeddings2)))

    for i, emb1 in enumerate(embeddings1):
        for j, emb2 in enumerate(embeddings2):
            if hasattr(emb1, 'values'):
                vec1 = emb1.values
            else:
                vec1 = emb1

            if hasattr(emb2, 'values'):
                vec2 = emb2.values
            else:
                vec2 = emb2

            similarity_matrix[i, j] = cosine_similarity(vec1, vec2)

    return similarity_matrix








def is_punct_text(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦å…¨ä¸ºæ ‡ç‚¹ç¬¦å·"""
    if not text:
        return False
    for ch in text:
        cat = unicodedata.category(ch)
        if not (cat.startswith('P') or cat.startswith('S')):
            return False
    return True








def find_optimal_split_semantic(result, forward_candidates, backward_candidates,
                               forward_english_sim, backward_english_sim):
    """
    æ‰¾åˆ°ç›®æ ‡å¥å­åˆ†å‰²çš„æœ€ä¼˜åˆ†å‰²ç‚¹ï¼ˆåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰

    å‚æ•°:
        result: ç›®æ ‡å¥å­åˆ†å‰²åˆ†è¯åçš„æ–‡æœ¬
        forward_candidates: ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)åˆ—è¡¨
        backward_candidates: ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘)åˆ—è¡¨
        forward_english_sim: ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)ä¸æºè‹±æ–‡ç¬¬ä¸€å¥çš„è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ
        backward_english_sim: ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘)ä¸æºè‹±æ–‡æœ€åä¸€å¥çš„è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ

    è¿”å›:
        å…ƒç»„ (split_index, forward_index, backward_index, scores)
        å…¶ä¸­scoresæ˜¯ä¸€ä¸ªåŒ…å«å„é¡¹å¾—åˆ†çš„å­—å…¸
    """
    n = len(result)
    best_total_score = -1
    best_split = None
    best_scores = None

    # å°è¯•æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹
    for split_idx in range(1, n):
        # è·å–å½“å‰åˆ†å‰²ç‚¹çš„å‰å‘å’Œåå‘éƒ¨åˆ†
        forward_tokens = result[:split_idx]
        backward_tokens = result[split_idx:]

        # å¦‚æœä»»ä¸€éƒ¨åˆ†å¤ªçŸ­ï¼ˆå°‘äº2ä¸ªtokenï¼‰ï¼Œåˆ™è·³è¿‡
        if len(forward_tokens) < 2 or len(backward_tokens) < 2:
            continue

        # æ‰¾åˆ°å¯¹åº”çš„å€™é€‰ç´¢å¼•
        forward_idx = len(forward_tokens) - 2  # ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)ä»é•¿åº¦2å¼€å§‹ï¼Œ0ç´¢å¼•
        backward_idx = len(backward_tokens) - 2  # ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘)ä»é•¿åº¦2å¼€å§‹ï¼Œ0ç´¢å¼•

        # å¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œåˆ™è·³è¿‡
        if (forward_idx < 0 or forward_idx >= len(forward_english_sim) or
                backward_idx < 0 or backward_idx >= len(backward_english_sim)):
            continue

        # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†
        forward_semantic = forward_english_sim[forward_idx, 0]
        backward_semantic = backward_english_sim[backward_idx, 0]

        # è®¡ç®—æ€»å¾—åˆ†ï¼ˆçº¯è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
        total_score = forward_semantic + backward_semantic

        # å¦‚æœå¾—åˆ†æ›´é«˜ï¼Œåˆ™æ›´æ–°æœ€ä½³åˆ†å‰²ç‚¹
        if total_score > best_total_score:
            best_total_score = total_score
            best_split = (split_idx, forward_idx, backward_idx)
            best_scores = {
                'forward_semantic': forward_semantic,
                'backward_semantic': backward_semantic,
                'total_score': total_score
            }

    if best_split:
        return best_split[0], best_split[1], best_split[2], best_scores
    else:
        return None, None, None, None


def print_embeddings(embeddings, text_list, type_name):
    """ä»¥æ ¼å¼åŒ–æ–¹å¼æ‰“å°åµŒå…¥å‘é‡"""
    print(f"\n{type_name}çš„åµŒå…¥å‘é‡:")

    for i, embedding in enumerate(embeddings):
        print(f"{type_name} {i + 1} '{text_list[i]}' çš„åµŒå…¥å‘é‡:")
        print(f"å‘é‡ç»´åº¦: {len(embedding.values)}")
        print(f"å‰10ä¸ªå€¼: {embedding.values[:10]}")
        print("-" * 50)


def print_similarity_matrix(matrix, row_labels, col_labels, name):
    """æ‰“å°ç›¸ä¼¼åº¦çŸ©é˜µ"""
    print(f"\n{name}ç›¸ä¼¼åº¦çŸ©é˜µ:")
    for i, row in enumerate(matrix):
        for j, val in enumerate(row):
            print(f"{row_labels[i]} ä¸ {col_labels[j]} çš„ç›¸ä¼¼åº¦: {val:.4f}")


def handle_middle_sentences(Source_sentences_en, chinese_text, best_forward_text, best_backward_text):
    """å¤„ç†ä¸­é—´æºè‹±æ–‡å¥å­ä¸å‰©ä½™ç›®æ ‡å¥å­åˆ†å‰²å­—ç¬¦çš„å¯¹åº”é€»è¾‘"""
    if len(Source_sentences_en) <= 2:
        print("æºè‹±æ–‡å¥å­æ•°é‡ <= 2ï¼Œæ— éœ€å¤„ç†ä¸­é—´å¥å­")
        return None

    print(f"\n===== å¤„ç†ä¸­é—´æºè‹±æ–‡å¥å­ =====")

    # è®¡ç®—å‰©ä½™çš„ç›®æ ‡å¥å­åˆ†å‰²å­—ç¬¦ä¸²
    chinese_tokens = get_local_tokenization(chinese_text)
    forward_tokens = get_local_tokenization(best_forward_text)
    backward_tokens = get_local_tokenization(best_backward_text)

    # æ‰¾åˆ°å‰å‘å’Œåå‘éƒ¨åˆ†åœ¨åŸç›®æ ‡å¥å­åˆ†å‰²ä¸­çš„ä½ç½®
    forward_len = len(forward_tokens)
    backward_len = len(backward_tokens)

    if forward_len + backward_len < len(chinese_tokens):
        # è®¡ç®—å‰©ä½™çš„ä¸­é—´éƒ¨åˆ†
        middle_tokens = chinese_tokens[forward_len:len(chinese_tokens) - backward_len]
        middle_text = ''.join(middle_tokens)

        print(f"å‰©ä½™çš„ä¸­é—´ç›®æ ‡å¥å­åˆ†å‰²å­—ç¬¦: '{middle_text}'")

        # å¤„ç†ä¸­é—´çš„æºè‹±æ–‡å¥å­ï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªï¼‰
        middle_Source_sentences_en = Source_sentences_en[1:-1]
        print(f"ä¸­é—´çš„æºè‹±æ–‡å¥å­: {middle_Source_sentences_en}")

        # è®¡ç®—ä¸­é—´æºè‹±æ–‡å¥å­ä¸ä¸­é—´ç›®æ ‡å¥å­åˆ†å‰²å­—ç¬¦çš„ç›¸ä¼¼åº¦
        results = []
        for i, eng_clause in enumerate(middle_Source_sentences_en):
            semantic_sim_emb = get_qwen_embeddings([eng_clause, middle_text])
            semantic_similarity = cosine_similarity(semantic_sim_emb[0].values, semantic_sim_emb[1].values)

            results.append({
                'english_clause': eng_clause,
                'semantic_similarity': semantic_similarity
            })

            print(f"æºè‹±æ–‡ä¸­é—´å¥å­ {i + 1}: '{eng_clause}'")
            print(f"  è¯­ä¹‰ç›¸ä¼¼åº¦: {semantic_similarity:.4f}")

        return {
            'middle_chinese_text': middle_text,
            'middle_english_results': results
        }
    else:
        print("å‰å‘å’Œåå‘éƒ¨åˆ†å·²è¦†ç›–æ•´ä¸ªç›®æ ‡å¥å­åˆ†å‰²å­—ç¬¦ä¸²ï¼Œæ— ä¸­é—´éƒ¨åˆ†")
        return None



def analyze_tokenization_process():
    """åˆ†ætokenå¤„ç†æµç¨‹"""
    print("=" * 70)
    print("ğŸ“– tokenåˆ†è¯å¤„ç†æµç¨‹")
    print("=" * 70)
    
    test_text = "the everyday citizens in these communities"
    print(f"ğŸ“ åˆ†ææ–‡æœ¬: '{test_text}'")
    print("-" * 70)
    
    # tokenåˆ†è¯è¿‡ç¨‹
    print("ğŸ”„ tokenåˆ†è¯è¿‡ç¨‹:")
    tokens = get_local_tokenization(test_text)
    
    print(f"   1. æ¨¡å‹åˆ†è¯å™¨å¤„ç†æ–‡æœ¬")
    print(f"   2. è½¬æ¢ä¸ºtokenå­—ç¬¦ä¸²ï¼ˆä¸å«ç‰¹æ®Šæ ‡è®°ï¼‰")
    print(f"   3. è§£ç tokenä¸ºå¯è¯»æ–‡æœ¬")
    
    print(f"\nğŸ”¤ æœ€ç»ˆtokenç»“æœ:")
    print(f"   æ•°é‡: {len(tokens)} ä¸ªtoken")
    print(f"   å†…å®¹: {' | '.join(tokens)}")
    
    # æ˜¾ç¤ºæ¯ä¸ªtokençš„è§£ç è¿‡ç¨‹
    print(f"\nğŸ“‹ tokenè§£ç è¯¦æƒ…:")
    inputs = tokenizer(test_text, padding=False,
                      truncation=True, max_length=512, add_special_tokens=False)
    raw_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'])
    
    for i, token in enumerate(raw_tokens):
        decoded = decode_token(token)
        print(f"   [{i+1}] '{token}' â†’ '{decoded}'")
    
    print("\n" + "=" * 70)


def main():
    print("=" * 60)
    print("å¼€å§‹ä¸»ç¨‹åº")
    print("=" * 60)

    Source_sentences_en = [
        "in which the everyday citizens in these communities",
        "contribute to the projects that are in the campaign"
    ]

    Source_sentences_zh = [
        "è¿™äº›ç¤¾åŒºçš„æ™®é€šå±…æ°‘",
        "ä¸ºæ´»åŠ¨ä¸­çš„é¡¹ç›®ææ¬¾"
    ]

    Target_sentence_segmentation = "ç”±ç¤¾åŒºé‡Œçš„æ™®é€šå¸‚æ°‘ä¸ºè¿™äº›æ´»åŠ¨ä¸­çš„é¡¹ç›®å‡ºèµ„"

    # ä½¿ç”¨æœ¬åœ° XLM-RoBERTa-XL æ¨¡å‹è¿›è¡Œç›®æ ‡å¥å­åˆ†å‰²åˆ†è¯
    result = get_local_tokenization(Target_sentence_segmentation)
    print(f"ğŸ€„ ç›®æ ‡å¥å­åˆ†å‰²: '{Target_sentence_segmentation}'")
    print(f"ğŸ”¤ [tokenåˆ†è¯] ç›®æ ‡å¥å­åˆ†å‰²åˆ†è¯: {' | '.join(result)}")
    print(f"   Tokenæ•°é‡: {len(result)}")
    print('=' * 70)

    # åˆ›å»ºä»å‰å‘åçš„ç›®æ ‡å¥å­å€™é€‰å­å¥
    forward_candidates = []
    for i in range(2, len(result) + 1):
        sub_clause_list = result[0:i]
        sub_clause_str = ''.join(sub_clause_list)
        forward_candidates.append(sub_clause_str)

    # åˆ›å»ºä»åå‘å‰çš„ç›®æ ‡å¥å­å€™é€‰å­å¥
    backward_candidates = []
    for i in range(2, len(result) + 1):
        sub_clause_list = result[-i:]
        sub_clause_str = ''.join(sub_clause_list)
        backward_candidates.append(sub_clause_str)

    # æ‰“å°ç›®æ ‡å¥å­å€™é€‰å­å¥ä¿¡æ¯
    print(f"\nä»å‰å‘åæ€»å…±ç”Ÿæˆäº† {len(forward_candidates)} ä¸ªç›®æ ‡å¥å­å€™é€‰å­å¥")
    for i, candidate in enumerate(forward_candidates):
        print(f"ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘) {i + 1}: {candidate}")

    print(f"\nä»åå‘å‰æ€»å…±ç”Ÿæˆäº† {len(backward_candidates)} ä¸ªç›®æ ‡å¥å­å€™é€‰å­å¥")
    for i, candidate in enumerate(backward_candidates):
        print(f"ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘) {i + 1}: {candidate}")

    print("\nå¼€å§‹å¤„ç†åµŒå…¥å‘é‡...")

    # ä½¿ç”¨ Qwen3-Embedding-8B æ¨¡å‹åµŒå…¥æºè‹±æ–‡å¥å­
    print("\nå¤„ç†æºè‹±æ–‡å¥å­çš„åµŒå…¥å‘é‡...")
    english_embeddings = get_qwen_embeddings(Source_sentences_en)
    print_embeddings(english_embeddings, Source_sentences_en, "æºè‹±æ–‡å¥å­")

    # åµŒå…¥ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)
    print("\nå¤„ç†ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)çš„åµŒå…¥å‘é‡...")
    forward_embeddings = get_qwen_embeddings(forward_candidates)
    print_embeddings(forward_embeddings, forward_candidates, "ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)")

    # åµŒå…¥ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘)
    print("\nå¤„ç†ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘)çš„åµŒå…¥å‘é‡...")
    backward_embeddings = get_qwen_embeddings(backward_candidates)
    print_embeddings(backward_embeddings, backward_candidates, "ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘)")

    # ===== å‰å‘å¤„ç† =====
    print("\n===== ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)å¤„ç† =====")

    # ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)ä¸æºè‹±æ–‡ç¬¬ä¸€å¥çš„å¥å­åµŒå…¥ç›¸ä¼¼åº¦
    print("\n--- ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)ä¸æºè‹±æ–‡ç¬¬ä¸€å¥ ---")

    # è®¡ç®—ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)ä¸æºè‹±æ–‡ç¬¬ä¸€å¥çš„è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ
    print("\nè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ...")
    print("ä½¿ç”¨Qwen3-Embedding-8Bæ¨¡å‹è®¡ç®—ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)ä¸æºè‹±æ–‡ç¬¬ä¸€å¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦")
    forward_english_sim = calculate_similarity_matrix(forward_embeddings, [english_embeddings[0]])

    print("\nğŸ’¯ è®¡ç®—å¥å­åµŒå…¥ç›¸ä¼¼åº¦...")
    for i, candidate in enumerate(forward_candidates):
        similarity = forward_english_sim[i, 0]
        print(f"ğŸ“Š ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘){i + 1} â†’ æºè‹±æ–‡ç¬¬ä¸€å¥")
        print(f"   ğŸ“ˆ åµŒå…¥ç›¸ä¼¼åº¦: {similarity:.4f}")
        print(f"   ğŸ’¬ '{candidate}' â†” '{Source_sentences_en[0]}'")

    # ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)ä¸æºä¸­æ–‡ç¬¬ä¸€å¥çš„å¥å­åµŒå…¥ç›¸ä¼¼åº¦
    print("\n--- ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘)ä¸æºä¸­æ–‡ç¬¬ä¸€å¥ ---")
    chinese_first_embedding = get_qwen_embeddings([Source_sentences_zh[0]])
    forward_chinese_sim = calculate_similarity_matrix(forward_embeddings, chinese_first_embedding)

    print("\nğŸ’¯ è®¡ç®—å¥å­åµŒå…¥ç›¸ä¼¼åº¦...")
    for i, candidate in enumerate(forward_candidates):
        similarity = forward_chinese_sim[i, 0]
        print(f"ğŸ“Š ç›®æ ‡å¥å­å€™é€‰å­å¥(å‰å‘){i + 1} â†’ æºä¸­æ–‡ç¬¬ä¸€å¥")
        print(f"   ğŸ“ˆ åµŒå…¥ç›¸ä¼¼åº¦: {similarity:.4f}")
        print(f"   ğŸ’¬ '{candidate}' â†” '{Source_sentences_zh[0]}'")

    # ===== åå‘å¤„ç† =====
    print("\n===== ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘)å¤„ç† =====")

    # ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘)ä¸æºè‹±æ–‡æœ€åä¸€å¥çš„å¥å­åµŒå…¥ç›¸ä¼¼åº¦
    print("\n--- ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘)ä¸æºè‹±æ–‡æœ€åä¸€å¥ ---")
    backward_english_sim = calculate_similarity_matrix(backward_embeddings, [english_embeddings[-1]])

    print("\nğŸ’¯ è®¡ç®—å¥å­åµŒå…¥ç›¸ä¼¼åº¦...")
    for i, candidate in enumerate(backward_candidates):
        similarity = backward_english_sim[i, 0]
        print(f"ğŸ“Š ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘){i + 1} â†’ æºè‹±æ–‡æœ€åä¸€å¥")
        print(f"   ğŸ“ˆ åµŒå…¥ç›¸ä¼¼åº¦: {similarity:.4f}")
        print(f"   ğŸ’¬ '{candidate}' â†” '{Source_sentences_en[-1]}'")

    # ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘)ä¸æºä¸­æ–‡æœ€åä¸€å¥çš„å¥å­åµŒå…¥ç›¸ä¼¼åº¦
    print("\n--- ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘)ä¸æºä¸­æ–‡æœ€åä¸€å¥ ---")
    chinese_last_embedding = get_qwen_embeddings([Source_sentences_zh[-1]])
    backward_chinese_sim = calculate_similarity_matrix(backward_embeddings, chinese_last_embedding)

    print("\nğŸ’¯ è®¡ç®—å¥å­åµŒå…¥ç›¸ä¼¼åº¦...")
    for i, candidate in enumerate(backward_candidates):
        similarity = backward_chinese_sim[i, 0]
        print(f"ğŸ“Š ç›®æ ‡å¥å­å€™é€‰å­å¥(åå‘){i + 1} â†’ æºä¸­æ–‡æœ€åä¸€å¥")
        print(f"   ğŸ“ˆ åµŒå…¥ç›¸ä¼¼åº¦: {similarity:.4f}")
        print(f"   ğŸ’¬ '{candidate}' â†” '{Source_sentences_zh[-1]}'")


if __name__ == "__main__":
    main()